{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### LOOP   window = 18   ########################\n",
    "\n",
    "# Attention-BLSTM classification of VS+VM vs. CL\n",
    "\n",
    "'''\n",
    "To prevent Colab from time out, go to the google Colab console (ctrl+shift+i) and type:\n",
    "function ClickConnect(){console.log(\"Working\");document.querySelector(\"colab-toolbar-button#connect\").click()}setInterval(ClickConnect,60000)\n",
    "\n",
    "Don't exit the console until you get \"Working\" as the output in the console window. \n",
    "It would keep on clicking the page and prevent it from disconnecting.\n",
    "Make sure you dont run anything for more than 12 hrs on Colab!\n",
    "'''\n",
    "##########################################\n",
    "import numpy\n",
    "from numpy.random import seed\n",
    "seed(2)\n",
    "!pip install tensorflow\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(2)\n",
    "# set_random_seed(2)\n",
    "\n",
    "\n",
    "############################################\n",
    "# import the required modules\n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.callbacks import EarlyStopping\n",
    "#from keras.optimizers import Adamax\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize,LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Attention implemented by https://github.com/CyberZHG/keras-self-attention\n",
    "!pip install keras-self-attention \n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "# experiment options\n",
    "# windows = [4, 6, 8]\n",
    "windows = [4]\n",
    "modes = ['switch']\n",
    "\n",
    "\n",
    "# number of features for switch / ps / as tests\n",
    "nb_switch = 8\n",
    "nb_ps = 1\n",
    "nb_as = 1\n",
    "nb_multi = nb_switch + nb_ps + nb_as      #######\n",
    "\n",
    "# lstm_sizes = [[8,4,2],[16,8,4],[32,16,8],[128,64,32],[256,128,64],[512,256,128],[1024,512,256]]\n",
    "\n",
    "\n",
    "# BLSTM variables\n",
    "# lstm_sizes = [[8,4,2],[16,8,4],[32,16,8],[128,64,32]] # number of neurons in the BLSTM layers\n",
    "time_steps = [1]  # input history to include\n",
    "attention_widths = [1] # width of the local context for the attention layer\n",
    "lstm_sizes = [[128,64,32]]\n",
    "# other parameters\n",
    "batch_size = 32 # for estimating error gradient\n",
    "nb_class = 2 # number of classes\n",
    "nb_epoch = 50 # number of total epochs to train the model\n",
    "\n",
    "# optimization function\n",
    "opt_func = Adamax(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "# to prevent over-fitting\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=20)\n",
    "\n",
    "# access data from Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# dir_log = '/content/drive/My Drive/FIT5126-Minor-Thesis/outputs/exp3/binary_'\n",
    "dir_data = '/content/drive/My Drive/FIT5126-Minor-Thesis/data_Faezeh/processed/window_0nan/'\n",
    "# printout = '/content/drive/My Drive/FIT5126-Minor-Thesis/outputs/logs/binary_printout_'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# turn off the warnings, be careful when use this\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# # save outputs to a log file\n",
    "# ts = time.time()\n",
    "# printout_f = printout + str(ts) + '.txt'\n",
    "# idlestdout = sys.stdout\n",
    "# logger = open(printout_f, \"w\")\n",
    "# sys.stdout = logger\n",
    "\n",
    "# reshape panda.DataFrame to Keras style: (batch_size, time_step, nb_features)\n",
    "def reshape_data(data, n_prev):\n",
    "    docX = []\n",
    "    for i in range(len(data)):\n",
    "        if i < (len(data)-n_prev):\n",
    "            docX.append(data.iloc[i:i+n_prev].values)\n",
    "        else: # the frames in the last window use the same context\n",
    "            docX.append(data.iloc[(len(data)-n_prev):len(data)].values)\n",
    "    alsX = np.array(docX)\n",
    "    return alsX\n",
    "\n",
    "# define the BLSTM model with attention\n",
    "def attBLSTM(lstm_size, attention_width, nb_class, opt_func):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units=lstm_size[0], return_sequences=True))) # BLSTM layer 1\n",
    "    model.add(Bidirectional(LSTM(units=lstm_size[1], return_sequences=True))) # BLSTM layer 2\n",
    "    model.add(Bidirectional(LSTM(units=lstm_size[2], return_sequences=True))) # BLSTM layer 3\n",
    "    # model.add(Bidirectional(LSTM(units=lstm_size[3], return_sequences=True))) # BLSTM layer 4\n",
    "    # model.add(Bidirectional(LSTM(units=lstm_size[4], return_sequences=True))) # BLSTM layer 5\n",
    "    model.add(SeqSelfAttention(attention_width=attention_width, attention_activation='sigmoid')) # attention layer\n",
    "    model.add(Dense(units=nb_class, activation='softmax')) # output layer, predict emotion dimensions seperately\n",
    "    return model\n",
    "\n",
    "# experiments\n",
    "count = 1\n",
    "for window in windows:       \n",
    "# for dataset in sets:\n",
    "    for mode in modes: \n",
    "        # data files\n",
    "        # file_log = dir_log + 'aln_' + 'window_' + str(window) + '_' + mode + '_log.txt'   #########\n",
    "        file_data_train = dir_data + str(window) + '/combined_train_'  + mode + '.csv'        #########\n",
    "        file_data_dev = dir_data + str(window) + '/combined_dev_'  + mode + '.csv'          #########\n",
    "        file_data_test = dir_data + str(window) + '/combined_test_' + mode + '.csv'         #########\n",
    "\n",
    "        nb_features = globals()[str('nb_' + mode)] * window\n",
    "\n",
    "        \n",
    "        # read in data\n",
    "        data_train = pd.read_csv(file_data_train, usecols=range(nb_features), na_values = ' ', header=None)            #########\n",
    "        data_dev = pd.read_csv(file_data_dev, usecols=range(nb_features), na_values = ' ', header=None)                #########\n",
    "        data_test = pd.read_csv(file_data_test, usecols=range(nb_features), na_values = ' ', header=None)              #########\n",
    "        \n",
    "        label_data_train = pd.read_csv(file_data_train, usecols=[(nb_features)], header=None)                        #########\n",
    "        label_raw_data_train = label_data_train.values                                                                  #########\n",
    "        \n",
    "        label_data_dev = pd.read_csv(file_data_dev , usecols=[(nb_features)], header=None)                            #########\n",
    "        label_raw_data_dev  = label_data_dev .values                                                                    #########\n",
    "        \n",
    "        label_data_test = pd.read_csv(file_data_test , usecols=[(nb_features)], header=None)                           #########\n",
    "        label_raw_data_test  = label_data_test .values                                                                   #########\n",
    "        \n",
    "        # one-hot encoding of the classes for train dataset\n",
    "        label_conv_train = []\n",
    "        for item in label_raw_data_train:\n",
    "            if item == 'CL':\n",
    "                converted_label = [1,0] # CL\n",
    "            else:\n",
    "                converted_label = [0,1] # VS or VM\n",
    "            label_conv_train.append(converted_label)\n",
    "        label_conv_train = np.asarray(label_conv_train)\n",
    "        label_conv_train_df = pd.DataFrame(label_conv_train, index=None)\n",
    "\n",
    "        # one-hot encoding of the classes for development dataset\n",
    "        label_conv_dev = []\n",
    "        for item in label_raw_data_dev:\n",
    "            if item == 'CL':\n",
    "                converted_label = [1,0] # CL\n",
    "            else:\n",
    "                converted_label = [0,1] # VS or VM\n",
    "            label_conv_dev.append(converted_label)\n",
    "        label_conv_dev = np.asarray(label_conv_dev)\n",
    "        label_conv_dev_df = pd.DataFrame(label_conv_dev, index=None)\n",
    "\n",
    "\n",
    "        # one-hot encoding of the classes for test dataset\n",
    "        label_conv_test = []\n",
    "        for item in label_raw_data_test:\n",
    "            if item == 'CL':\n",
    "                converted_label = [1,0] # CL\n",
    "            else:\n",
    "                converted_label = [0,1] # VS or VM\n",
    "            label_conv_test.append(converted_label)\n",
    "        label_conv_test = np.asarray(label_conv_test)\n",
    "        label_conv_test_df = pd.DataFrame(label_conv_test, index=None)\n",
    "\n",
    "\n",
    "        # Grid search for best parameters\n",
    "        para_list = []\n",
    "        tst_pred_list = []\n",
    "        for time_step in time_steps:\n",
    "            X_train = reshape_data(data_train, time_step) # pad feature data\n",
    "            y_train = reshape_data(label_conv_train_df, time_step) # pad label data\n",
    "\n",
    "            X_dev = reshape_data(data_dev, time_step) # pad feature data\n",
    "            y_dev = reshape_data(label_conv_dev_df, time_step) # pad label data\n",
    "\n",
    "            X_test = reshape_data(data_test, time_step) # pad feature data\n",
    "            y_test = reshape_data(label_conv_test_df, time_step) # pad label data\n",
    "\n",
    "            for lstm_size in lstm_sizes:\n",
    "                for attention_width in attention_widths:\n",
    "                    para_list.append([time_step, lstm_size, attention_width]) # save parameter set \n",
    "                    print('\\n================================ No. %s of 16 ========================================' % count)                   \n",
    "                    print('\\n Tranining on:%s ,dataset\\n' % (mode) )\n",
    "                    # build model with given parameters\n",
    "                    model = attBLSTM(lstm_size, attention_width, nb_class, opt_func)\n",
    "                    model.compile(loss='categorical_crossentropy', optimizer=opt_func, metrics=['categorical_accuracy'])\n",
    "                    # training the model\n",
    "                    #x = np.asarray(x).astype('float32')\n",
    "                    model.fit(X_train, y_train, batch_size=batch_size, epochs=nb_epoch, validation_data=(X_dev, y_dev), verbose=0)\n",
    "                    # evaluation\n",
    "                    model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "\n",
    "                    # print confusion matrix\n",
    "                    y_test_non_category = [ np.argmax(t[0]) for t in y_test ]\n",
    "                    tst_pred = model.predict(X_test)\n",
    "                    y_predict_non_category = [ np.argmax(t[0]) for t in tst_pred ]\n",
    "\n",
    "                    tst_f1 = f1_score(y_test_non_category, y_predict_non_category, average='weighted')\n",
    "                    print('\\nParameters: mode = %s, time_step = %s, [h1, h2, h3] = %s, attention_width = %s\\n' % (mode, time_step, lstm_size, attention_width))\n",
    "                    print('Confusion Matrix on test set')\n",
    "                    print(confusion_matrix(y_test_non_category, y_predict_non_category))\n",
    "                    print('Weighted F1-score on test set:', tst_f1)\n",
    "                    count += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "!pip install lime\n",
    "import lime\n",
    "from lime import lime_tabular\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Attention implemented by https://github.com/CyberZHG/keras-self-attention\n",
    "!pip install keras-self-attention \n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "#eli5 package (https://eli5.readthedocs.io/en/latest)\n",
    "!pip install eli5\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "#lime package (https://github.com/marcotcr/lime)\n",
    "import lime\n",
    "import lime.lime_tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PredFunc (XTest):\n",
    "  tst_pred = model.predict(XTest)\n",
    "\n",
    "  ff = [ t[0] for t in tst_pred ]\n",
    "  \n",
    "  list_ff = []\n",
    "  for item in ff:\n",
    "    item = item.tolist()\n",
    "    list_ff.append(item)\n",
    "  \n",
    "  ff_aray = np.array(list_ff)\n",
    "  ff_aray\n",
    "  \n",
    "  return(ff_aray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['1', '2','3','4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17','18','19','20','21','22','23','24','25','26','27','28','29','30','31','32']\n",
    "random_state=123\n",
    "\n",
    "explainer = lime_tabular.RecurrentTabularExplainer(X_train, training_labels=y_train, feature_names=header, random_state=random_state,\n",
    "                                                   discretize_continuous=True,\n",
    "                                                   class_names=['Healthy', 'Patient'],\n",
    "                                                   discretizer='decile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_test[0])\n",
    "print(y_test[0])\n",
    "\n",
    "print('\\n\\n--------------------\\n\\n')\n",
    "\n",
    "y_predict = np.argmax(model.predict(X_test), axis=-1)\n",
    "print(y_predict[0])\n",
    "\n",
    "print('\\n\\n--------------------\\n\\n')\n",
    "\n",
    "predictions = (model.predict(X_test) < 0.5).astype(\"int32\")\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SWITCH-VS-VS\n",
    "\n",
    "exp = explainer.explain_instance(X_test[372], PredFunc, num_features=10, labels=(1,))\n",
    "exp.show_in_notebook(show_table=True)\n",
    "plt = exp.as_pyplot_figure()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SWITCH-VS-CL\n",
    "\n",
    "exp = explainer.explain_instance(X_test[837], PredFunc, num_features=10, labels=(1,))\n",
    "exp.show_in_notebook(show_table=True)\n",
    "plt = exp.as_pyplot_figure()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SWITCH-CL-CL\n",
    "\n",
    "exp = explainer.explain_instance(X_test[93], PredFunc, num_features=10, labels=(1,))\n",
    "exp.show_in_notebook(show_table=True)\n",
    "plt = exp.as_pyplot_figure()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SWITCH-CL-VS\n",
    "\n",
    "exp = explainer.explain_instance(X_test[0], PredFunc, num_features=10, labels=(1,))\n",
    "exp.show_in_notebook(show_table=True)\n",
    "plt = exp.as_pyplot_figure()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
